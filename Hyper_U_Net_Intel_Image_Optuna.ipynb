{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VV-IZUO3YrT3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import math\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from numpy import zeros, ones, vstack, hstack\n",
        "from numpy.random import permutation\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.utils import shuffle\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8j1wwG6Y5nx",
        "outputId": "225dc6af-8e03-4db3-b5b9-2adab2f2fd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: fratzcan\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
            "Downloading intel-image-classification.zip to ./intel-image-classification\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 346M/346M [00:03<00:00, 120MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets --quiet,\n",
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/puneet6060/intel-image-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw06kUoFY7dw",
        "outputId": "f6dfd6f9-ad33-4f0a-c7bd-111d27df0542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "_30hJS5HY9kL",
        "outputId": "028be849-a981-4330-e4c4-04ef950f36f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfiratozc\u001b[0m (\u001b[33mfiratozc-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aygV8NGZZoy"
      },
      "outputs": [],
      "source": [
        "def RGB2LAB2(R0, G0, B0):\n",
        "    R = R0 / 255\n",
        "    G = G0 / 255\n",
        "    B = B0 / 255\n",
        "\n",
        "    Y = 0.299*R + 0.587*G + 0.114*B\n",
        "    X = 0.449*R + 0.353*G + 0.198*B\n",
        "    Z = 0.012*R + 0.089*G + 0.899*B\n",
        "\n",
        "    L = Y\n",
        "    a = (X - Y) / 0.234\n",
        "    b = (Y - Z) / 0.785\n",
        "\n",
        "    return L, a, b\n",
        "\n",
        "\n",
        "def LAB22RGB(L, a, b, device):\n",
        "    a11, a12, a13 = 0.299, 0.587, 0.114\n",
        "    a21, a22, a23 = (0.15/0.234), (-0.234/0.234), (0.084/0.234)\n",
        "    a31, a32, a33 = (0.287/0.785), (0.498/0.785), (-0.785/0.785)\n",
        "\n",
        "    aa_np = np.array([[a11, a12, a13], [a21, a22, a23], [a31, a32, a33]])\n",
        "    aa_inv_tensor = torch.tensor(np.linalg.inv(aa_np), dtype=torch.float32).to(device)\n",
        "\n",
        "    Lab = torch.cat((L, a, b), dim=1) \n",
        "\n",
        "    batch_size, _, H, W = Lab.shape\n",
        "\n",
        "    Lab_reshaped = Lab.view(batch_size, 3, -1)\n",
        "\n",
        "    Lab_permuted = Lab_reshaped.permute(0, 2, 1) \n",
        "\n",
        "    rgb_permuted = torch.matmul(Lab_permuted, aa_inv_tensor.t()) \n",
        "\n",
        "    rgb_reshaped = rgb_permuted.permute(0, 2, 1)\n",
        "    rgb_image = rgb_reshaped.view(batch_size, 3, H, W)\n",
        "\n",
        "    rgb_image = torch.clamp(rgb_image, 0.0, 1.0)\n",
        "\n",
        "    return rgb_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jyhlbi-wZi22"
      },
      "outputs": [],
      "source": [
        "def psnr(img1, img2):\n",
        "    mse = np.mean((img1.astype(\"float\") - img2.astype(\"float\")) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    PIXEL_MAX = 255.0\n",
        "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
        "\n",
        "def mse(imageA, imageB, bands):\n",
        "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        "    err /= float(imageA.shape[0] * imageA.shape[1] * bands)\n",
        "    return err\n",
        "\n",
        "def mae(imageA, imageB, bands):\n",
        "    err = np.sum(np.abs((imageA.astype(\"float\") - imageB.astype(\"float\"))))\n",
        "    err /= float(imageA.shape[0] * imageA.shape[1] * bands)\n",
        "    return err\n",
        "\n",
        "def rmse(imageA, imageB, bands):\n",
        "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
        "    err /= float(imageA.shape[0] * imageA.shape[1] * bands)\n",
        "    err = np.sqrt(err)\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HPHir0CdZtfi"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Double Convolution Block\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k69ZPSllZvSG"
      },
      "outputs": [],
      "source": [
        "class TripleConv(nn.Module):\n",
        "    \"\"\"Triple Convolution Block\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TripleConv, self).__init__()\n",
        "        self.triple_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.triple_conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrEqzwbYZxUa"
      },
      "outputs": [],
      "source": [
        "class UNet1(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=2):\n",
        "        super(UNet1, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = DoubleConv(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv3 = TripleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv4 = TripleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv5 = TripleConv(512, 512)\n",
        "        self.pool5 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv55 = TripleConv(512, 512)\n",
        "\n",
        "        # Decoder\n",
        "        self.up66 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
        "        self.conv66 = DoubleConv(1024, 512)  # 512 + 512 from skip connection\n",
        "\n",
        "        self.up6 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
        "        self.conv6 = DoubleConv(1024, 512)  # 512 + 512 from skip connection\n",
        "\n",
        "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv7 = DoubleConv(512, 256)  # 256 + 256 from skip connection\n",
        "\n",
        "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv8 = DoubleConv(256, 128)  # 128 + 128 from skip connection\n",
        "\n",
        "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv9 = DoubleConv(128, 64)  # 64 + 64 from skip connection\n",
        "\n",
        "        # Multi-scale feature fusion\n",
        "        self.up_f02 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.up_f12 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Final layers\n",
        "        self.conv11 = nn.Conv2d(384, 128, kernel_size=3, padding=1)  # 64+64+128+128\n",
        "        self.relu11 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv12 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.relu12 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv13 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.relu13 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv14 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        conv1 = self.conv1(x)\n",
        "        x1 = self.pool1(conv1)\n",
        "\n",
        "        conv2 = self.conv2(x1)\n",
        "        x2 = self.pool2(conv2)\n",
        "\n",
        "        conv3 = self.conv3(x2)\n",
        "        x3 = self.pool3(conv3)\n",
        "\n",
        "        conv4 = self.conv4(x3)\n",
        "        x4 = self.pool4(conv4)\n",
        "\n",
        "        conv5 = self.conv5(x4)\n",
        "        x5 = self.pool5(conv5)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv55 = self.conv55(x5)\n",
        "\n",
        "        # Decoder\n",
        "        up66 = self.up66(conv55)\n",
        "        if up66.size()[2:] != conv5.size()[2:]:\n",
        "            up66 = F.interpolate(up66, size=conv5.size()[2:], mode=\"bilinear\", align_corners=True)\n",
        "        merge66 = torch.cat([conv5, up66], dim=1)\n",
        "        conv66 = self.conv66(merge66)\n",
        "\n",
        "        up6 = self.up6(conv66)\n",
        "        if up6.size()[2:] != conv4.size()[2:]:\n",
        "            up6 = F.interpolate(up6, size=conv4.size()[2:], mode=\"bilinear\", align_corners=True)\n",
        "        merge6 = torch.cat([conv4, up6], dim=1)\n",
        "        conv6 = self.conv6(merge6)\n",
        "\n",
        "        up7 = self.up7(conv6)\n",
        "        if up7.size()[2:] != conv3.size()[2:]:\n",
        "            up7 = F.interpolate(up7, size=conv3.size()[2:], mode=\"bilinear\", align_corners=True)\n",
        "        merge7 = torch.cat([conv3, up7], dim=1)\n",
        "        conv7 = self.conv7(merge7)\n",
        "\n",
        "        up8 = self.up8(conv7)\n",
        "        if up8.size()[2:] != conv2.size()[2:]:\n",
        "            up8 = F.interpolate(up8, size=conv2.size()[2:], mode=\"bilinear\", align_corners=True)\n",
        "        merge8 = torch.cat([conv2, up8], dim=1)\n",
        "        conv8 = self.conv8(merge8)\n",
        "\n",
        "        up9 = self.up9(conv8)\n",
        "        if up9.size()[2:] != conv1.size()[2:]:\n",
        "            up9 = F.interpolate(up9, size=conv1.size()[2:], mode=\"bilinear\", align_corners=True)\n",
        "        merge9 = torch.cat([conv1, up9], dim=1)\n",
        "        conv9 = self.conv9(merge9)\n",
        "\n",
        "\n",
        "        # Multi-scale feature fusion\n",
        "        up_f01 = conv1  \n",
        "        up_f11 = conv9  \n",
        "        up_f02 = self.up_f02(conv2)  \n",
        "        up_f12 = self.up_f12(conv8)  \n",
        "\n",
        "        merge11 = torch.cat([up_f01, up_f11, up_f02, up_f12], dim=1)\n",
        "\n",
        "        conv11 = self.relu11(self.conv11(merge11))\n",
        "        conv12 = self.relu12(self.conv12(conv11))\n",
        "        conv13 = self.relu13(self.conv13(conv12))\n",
        "        output = self.tanh(self.conv14(conv13))\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ETgKETMZyyW"
      },
      "outputs": [],
      "source": [
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, file_list, dim=150):\n",
        "        self.file_list = file_list # image paths\n",
        "        self.dim = dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (self.dim, self.dim))\n",
        "\n",
        "        sz0, sz1 = img.shape[:2]\n",
        "        R1 = img[:, :, 0].reshape(-1, 1)\n",
        "        G1 = img[:, :, 1].reshape(-1, 1)\n",
        "        B1 = img[:, :, 2].reshape(-1, 1)\n",
        "\n",
        "        L, A, B = RGB2LAB2(R1, G1, B1)\n",
        "\n",
        "        L = L.reshape(sz0, sz1, 1)\n",
        "        A = A.reshape(sz0, sz1)\n",
        "        B = B.reshape(sz0, sz1)\n",
        "\n",
        "        ab = np.stack([A, B], axis=2)\n",
        "\n",
        "        L_tensor = torch.FloatTensor(L).permute(2, 0, 1)  \n",
        "        ab_tensor = torch.FloatTensor(ab).permute(2, 0, 1)  \n",
        "\n",
        "        return L_tensor, ab_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJmS-ui6Z1b4"
      },
      "outputs": [],
      "source": [
        "def load_vgg16_weights(model):\n",
        "    \"\"\"Load pretrained VGG16 weights to U-Net encoder\"\"\"\n",
        "    vgg16 = models.vgg16(pretrained=True).to(device)\n",
        "    vgg_features = vgg16.features\n",
        "\n",
        "    with torch.no_grad():\n",
        "        rgb_weights = vgg_features[0].weight\n",
        "\n",
        "        gray_weights = rgb_weights.mean(dim=1, keepdim=True)  \n",
        "\n",
        "        # Set weights for first layer\n",
        "        model.conv1.double_conv[0].weight.data = gray_weights\n",
        "        model.conv1.double_conv[0].bias.data = vgg_features[0].bias.data\n",
        "\n",
        "        # Set weights for second conv in first block\n",
        "        model.conv1.double_conv[2].weight.data = vgg_features[2].weight.data\n",
        "        model.conv1.double_conv[2].bias.data = vgg_features[2].bias.data\n",
        "\n",
        "        # Second block\n",
        "        model.conv2.double_conv[0].weight.data = vgg_features[5].weight.data\n",
        "        model.conv2.double_conv[0].bias.data = vgg_features[5].bias.data\n",
        "        model.conv2.double_conv[2].weight.data = vgg_features[7].weight.data\n",
        "        model.conv2.double_conv[2].bias.data = vgg_features[7].bias.data\n",
        "\n",
        "        # Third block (first two convs)\n",
        "        model.conv3.triple_conv[0].weight.data = vgg_features[10].weight.data\n",
        "        model.conv3.triple_conv[0].bias.data = vgg_features[10].bias.data\n",
        "        model.conv3.triple_conv[2].weight.data = vgg_features[12].weight.data\n",
        "        model.conv3.triple_conv[2].bias.data = vgg_features[12].bias.data\n",
        "        model.conv3.triple_conv[4].weight.data = vgg_features[14].weight.data\n",
        "        model.conv3.triple_conv[4].bias.data = vgg_features[14].bias.data\n",
        "\n",
        "        # Fourth block\n",
        "        model.conv4.triple_conv[0].weight.data = vgg_features[17].weight.data\n",
        "        model.conv4.triple_conv[0].bias.data = vgg_features[17].bias.data\n",
        "        model.conv4.triple_conv[2].weight.data = vgg_features[19].weight.data\n",
        "        model.conv4.triple_conv[2].bias.data = vgg_features[19].bias.data\n",
        "        model.conv4.triple_conv[4].weight.data = vgg_features[21].weight.data\n",
        "        model.conv4.triple_conv[4].bias.data = vgg_features[21].bias.data\n",
        "\n",
        "        # Fifth block\n",
        "        model.conv5.triple_conv[0].weight.data = vgg_features[24].weight.data\n",
        "        model.conv5.triple_conv[0].bias.data = vgg_features[24].bias.data\n",
        "        model.conv5.triple_conv[2].weight.data = vgg_features[26].weight.data\n",
        "        model.conv5.triple_conv[2].bias.data = vgg_features[26].bias.data\n",
        "        model.conv5.triple_conv[4].weight.data = vgg_features[28].weight.data\n",
        "        model.conv5.triple_conv[4].bias.data = vgg_features[28].bias.data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgFocyXbZ3Ih"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Parameters\n",
        "    # dim = 150 # For Intel Image\n",
        "    dim = 256 # For Landscape Image\n",
        "    batch_size = 16\n",
        "    epochs_max = 10\n",
        "    max_nb_min = 3\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    # train_path = os.path.join(cwd, 'seg_train', '*.png')\n",
        "    # files_tr_list = glob.glob(train_path)\n",
        "\n",
        "    # N = len(files_tr_list)\n",
        "    # print(f'Number of training images: {N}')\n",
        "\n",
        "\n",
        "\n",
        "    base_path = \"/content/intel-image-classification/seg_train/seg_train\"\n",
        "\n",
        "    # For Intel-Image Dataset\n",
        "    classes = os.listdir(base_path)\n",
        "\n",
        "    image_paths = []\n",
        "\n",
        "    for cls in classes:\n",
        "        folder_path = os.path.join(base_path, cls, \"*.jpg\")\n",
        "        for file in glob.glob(folder_path):\n",
        "            image_paths.append(file)\n",
        "            if len(image_paths) >= 3000:\n",
        "                break\n",
        "        if len(image_paths) >= 3000:\n",
        "            break\n",
        "\n",
        "    # For Landscape Dataset\n",
        "    # image_paths = glob.glob(os.path.join(base_path, \"*.jpg\"))\n",
        "\n",
        "    print(\"Number of training images:\", len(image_paths))\n",
        "\n",
        "    # Create model\n",
        "    model = UNet1(in_channels=1, out_channels=2).to(device)\n",
        "\n",
        "    print('Loading VGG16 pretrained weights...')\n",
        "    load_vgg16_weights(model)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    dataset = ColorizationDataset(image_paths, dim=dim)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    wandb.init(project=\"image_colorization\", name=\"HyperUNet_experiment-Landscape-Image\",\n",
        "               config={\n",
        "                   \"batch_size\": batch_size,\n",
        "                   \"epochs\": epochs_max,\n",
        "                   \"learning_rate\": 1e-4,\n",
        "                   \"model_name\": \"HyperUNet-Intel-Image\",\n",
        "                   \"optimizer\": \"Adam\",\n",
        "                   \"loss_function\": \"L1Loss\",\n",
        "                   \"dataset\": \"Landscape\"\n",
        "               })\n",
        "\n",
        "    tr_acc = np.zeros((epochs_max, 2))\n",
        "    time_tr = np.zeros((epochs_max, 2))\n",
        "    mae_min = float('inf')\n",
        "    nb_min = 0\n",
        "    stop = 0\n",
        "\n",
        "    print('Starting training...')\n",
        "\n",
        "    for epoch in range(epochs_max):\n",
        "        if stop:\n",
        "            break\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # wandb.log({\"batch_loss\": loss.item()})\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.6f}')\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        wandb.log({\"epoch\": epoch+1, \"avg_loss\": avg_loss})\n",
        "\n",
        "        # Update tracking arrays\n",
        "        tr_acc[epoch, 0] = epoch\n",
        "        tr_acc[epoch, 1] = avg_loss\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        time_diff = end_time - start_time\n",
        "        time_tr[epoch, 0] = epoch\n",
        "        time_tr[epoch, 1] = time_diff.seconds\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs_max}, Average Loss: {avg_loss:.6f}, Time: {time_diff.seconds}s')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if avg_loss > mae_min:\n",
        "            nb_min += 1\n",
        "        else:\n",
        "            mae_min = avg_loss\n",
        "            nb_min = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), os.path.join(cwd, 'Hyper_U_NET_pytorch.pth'))\n",
        "            print(f'New best model saved with loss: {mae_min:.6f}')\n",
        "\n",
        "        if nb_min > max_nb_min:\n",
        "            stop = 1\n",
        "            print('Early stopping triggered')\n",
        "\n",
        "        # Learning rate scheduling \n",
        "        if epoch + 1 == 1:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 5e-5\n",
        "        elif epoch + 1 == 2:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 2e-5\n",
        "        elif epoch + 1 == 4:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 1e-5\n",
        "        elif epoch + 1 == 8:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 5e-6\n",
        "        elif epoch + 1 == 16:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 2e-6\n",
        "        elif epoch + 1 == 32:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 1e-6\n",
        "        elif epoch + 1 == 64:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 5e-7\n",
        "        elif epoch + 1 == 128:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 2e-7\n",
        "        elif epoch + 1 == 256:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 1e-7\n",
        "\n",
        "\n",
        "        np.save(os.path.join(cwd, 'tr_Acc_Hyper_U_NET_pytorch.npy'), tr_acc)\n",
        "        np.save(os.path.join(cwd, 'Tr_runtime_Hyper_U_NET_pytorch.npy'), time_tr)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fan_iQQBZ6C2"
      },
      "outputs": [],
      "source": [
        "def load_model_for_inference(model_path, device):\n",
        "    model = UNet1(in_channels=1, out_channels=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def inference(model, l_channel):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if len(l_channel.shape) == 3:\n",
        "            l_channel = l_channel.unsqueeze(0)  \n",
        "\n",
        "        l_tensor = torch.FloatTensor(l_channel).to(device)\n",
        "        ab_pred = model(l_tensor)\n",
        "\n",
        "        return ab_pred.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yQobBDuaZ7Dj"
      },
      "outputs": [],
      "source": [
        "if __name__ == 'main':\n",
        "  train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ps9iO89Z70I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnNu_iAndhCQ"
      },
      "source": [
        "### Optuna Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TuRRYLxd5g2",
        "outputId": "60ee644b-6cea-4a8b-a7b5-a7fe5c46fcf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.5 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sHPf9o9pdwET"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fgGyG66Z7wp",
        "outputId": "ab27d409-1a94-4e32-f0d1-8318ccf8c7dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:02:20,633] A new study created in memory with name: no-name-bf74153d-748a-407e-9a74-f4411eb5a266\n",
            "/tmp/ipython-input-2584549285.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 528M/528M [00:05<00:00, 104MB/s]\n",
            "[I 2025-08-28 07:07:30,517] Trial 0 finished with value: 0.060671099364757536 and parameters: {'dim': 150, 'batch_size': 16, 'lr': 0.000593811464694052}. Best is trial 0 with value: 0.060671099364757536.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:23:53,468] Trial 1 finished with value: 0.05790867072343826 and parameters: {'dim': 256, 'batch_size': 16, 'lr': 0.000160573097701746}. Best is trial 1 with value: 0.05790867072343826.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:27:26,098] Trial 2 finished with value: 0.05546035818637363 and parameters: {'dim': 128, 'batch_size': 32, 'lr': 6.184067768364118e-05}. Best is trial 2 with value: 0.05546035818637363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:30:55,325] Trial 3 finished with value: 0.0592499566929681 and parameters: {'dim': 128, 'batch_size': 32, 'lr': 0.00018620289924440656}. Best is trial 2 with value: 0.05546035818637363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:34:15,118] Trial 4 finished with value: 0.06035598320147348 and parameters: {'dim': 128, 'batch_size': 32, 'lr': 0.0007616391844161132}. Best is trial 2 with value: 0.05546035818637363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:49:54,660] Trial 5 finished with value: 0.06023477378487587 and parameters: {'dim': 256, 'batch_size': 16, 'lr': 0.0006930783540287476}. Best is trial 2 with value: 0.05546035818637363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:53:17,220] Trial 6 finished with value: 0.06050739591083829 and parameters: {'dim': 128, 'batch_size': 32, 'lr': 0.000524207534366071}. Best is trial 2 with value: 0.05546035818637363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 07:56:58,604] Trial 7 finished with value: 0.055366841286420825 and parameters: {'dim': 128, 'batch_size': 16, 'lr': 5.6008593690527854e-05}. Best is trial 7 with value: 0.055366841286420825.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 08:13:27,377] Trial 8 finished with value: 0.05722824528813362 and parameters: {'dim': 256, 'batch_size': 8, 'lr': 0.00014136731385907975}. Best is trial 7 with value: 0.055366841286420825.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-28 08:20:36,895] Trial 9 finished with value: 0.05699840810894966 and parameters: {'dim': 150, 'batch_size': 8, 'lr': 0.00012005467161378065}. Best is trial 7 with value: 0.055366841286420825.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'dim': 128, 'batch_size': 16, 'lr': 5.6008593690527854e-05}\n",
            "Best loss: 0.055366841286420825\n"
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    dim = trial.suggest_categorical(\"dim\", [128, 150, 256])\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    epochs_max = 5\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    base_path = \"/content/intel-image-classification/seg_train/seg_train\"\n",
        "    classes = os.listdir(base_path)\n",
        "\n",
        "    image_paths = []\n",
        "    for cls in classes:\n",
        "        folder_path = os.path.join(base_path, cls, \"*.jpg\")\n",
        "        for file in glob.glob(folder_path):\n",
        "            image_paths.append(file)\n",
        "            if len(image_paths) >= 2000:\n",
        "                break\n",
        "        if len(image_paths) >= 2000:\n",
        "            break\n",
        "\n",
        "    print(\"Number of training images:\", len(image_paths))\n",
        "\n",
        "    # Model\n",
        "    model = UNet1(in_channels=1, out_channels=2).to(device)\n",
        "    load_vgg16_weights(model)\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Dataset\n",
        "    dataset = ColorizationDataset(image_paths, dim=dim)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    mae_min = float('inf')\n",
        "    nb_min = 0\n",
        "    max_nb_min = 2\n",
        "\n",
        "    for epoch in range(epochs_max):\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "\n",
        "        if avg_loss > mae_min:\n",
        "            nb_min += 1\n",
        "        else:\n",
        "            mae_min = avg_loss\n",
        "            nb_min = 0\n",
        "\n",
        "        if nb_min > max_nb_min:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return mae_min\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    print(\"Best hyperparameters:\", study.best_params)\n",
        "    print(\"Best loss:\", study.best_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t814b9HHZ7um",
        "outputId": "8d65221c-1f0b-434c-ad10-d569500111b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK-kCY4iZ7sG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3L4s72KZ7qD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sBbPdbBZ7nq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eru5swniZ7lD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
